## 一、归一化的算法

目的：某一个特征不会对结果造成更大的影响，一般是在多特征同比重的情况下进行数据的归一化处理

- 通过对原始数据进行变换把数据映射到(默认为[0,1])之间

- 公式: 𝑋′=  (𝑥−𝑚𝑖𝑛)/(𝑚𝑎𝑥−𝑚𝑖𝑛)
  - 𝑋′′=𝑋′∗(𝑚𝑥−𝑚𝑖)+𝑚𝑖  该式用于指定结果float的范围，默认就是0到1，对应参数feature_range
  

注：作用于每一列，max为一列的最大值，min为一列的最小值,那么X’’
为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0


转化前：

| 特征1 | 特征2 | 特征3 | 特征4 |
| --- | --- | --- | --- |
| 90 | 2 | 10 | 40 |
| 60 | 4 | 15 | 45 |
| 75 | 3 | 13 | 46 |

转化成归一化公式，这里是第一步，第二步还要*(1-0) + 0：

| 特征1 | 特征2 | 特征3 | 特征4 |
| --------------- | ----------- | --------------- | --------------- |
| (90-60)/(90-60) | (2-2)/(4-2) | (10-10)/(15-10) | (40-40)/(46-40) |
| (60-60)/(90-60) | (4-2)/(4-2) | (15-10)/(15-10) | (45-40)/(46-40) |
| (75-60)/(90-60) | (3-2)/(4-2) | (13-10)/(15-10) | (46-40)/(46-40) |


归一化的缺点是对异常点没有很好地处理，导致异常点对结果的影响太大,所以有些数据需要进行标准化

## 二、标准化的算法

通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内

- 公式: 𝑋′=  (𝑥−mean)/𝜎

注：作用于每一列，mean为平均值，𝜎为标准差(考量数据的稳定性)
  - var成为方差(考量数据的稳定性) var =  ((𝑥1−𝑚𝑒𝑎𝑛)^2+(𝑥2−𝑚𝑒𝑎𝑛)^2+…)/𝑛
n每个特征的样本数
  - 𝜎= √var

处理后我们发现每一列的均值为0，标准差为1

## 三、监督学习与无监督学习，离散型和连续型

- 监督学习
  - 分类    k-近邻算法、贝叶斯分类、决策树与
随机森林、逻辑回归、神经网络等，用于离散型
  - 回归    线性回归、岭回归等，用于连续型
  - 标注    隐马尔可夫模型     (不做要求)
- 无监督学习
  - 聚类    k-means
  

监督学习（英语：Supervised learning），可以由输入数据中学
到或建立一个模型，并依此模式推测新的结果。输入数据是由
输入特征值和目标值所组成。函数的输出可以是一个连续的值
（称为回归），或是输出是有限个离散值（称作分类）。


无监督学习（英语：Supervised learning），可以由输入数据中
学到或建立一个模型，并依此模式推测新的结果。输入数据是
由输入特征值所组成。



- 概念：分类是监督学习的一个核心问题，在监督学习中，当输出变量取有限个离散值时，预测问题变成为分类问题。最基础的便是二分类问题，即判断是非，从两个类别中选择一个作为预测结果；

## 四、朴素贝叶斯
P(C|W) = ( P(W|C)P(C) )/( P(W) )
W为给定文档的特征值，C为文档类别
公式可以理解为:
P(C|F1,F2...) = ( P(F1,F2...|C)P(C) )/( P(F1,F2...) )
其中C可以是不同类别
拉普拉斯平滑系数用于防止概率为0的发生

精确率P:查的准
召回率N:查的全
其他分类标准: F1-score  F1 = 2TP/(2TP+FN+FP)
                          = (2*Precision*Recall)/(Precision+Recall)

